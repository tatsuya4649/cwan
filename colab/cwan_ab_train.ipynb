{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cwan_ab_train.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6K-Y1cYyqC4X",
        "E4QeiGrurivk",
        "iYfh9lNjrQWm",
        "6DnKn6x6pLlP",
        "O5qpetjrr2Vk"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uHdb0vRmNsF",
        "outputId": "4363c7a7-c873-4ecc-a61a-5a6e9245d173",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FClbmZZ7mvVp"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import pickle"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXO8CyK5nmF0"
      },
      "source": [
        "# if you have a model parameter's file, setting path\n",
        "_CWAN_L_PATH = None\n",
        "_CWAN_AB_PATH = None\n",
        "# training hyperparameters\n",
        "_LR = 1e-5\n",
        "_WEIGHT_DECAY = 0.05\n",
        "_BATCH_SIZE = 64\n",
        "_START_EPOCH = 0 + 1\n",
        "_EPOCH = 200\n",
        "# dataset parameter\n",
        "_ONE_FILE_SIZE = 2000"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0Cx0leSnVFU",
        "outputId": "eb3eecd9-d28f-4b62-87da-888cb1451640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print('device => {}'.format(device))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device => cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K-Y1cYyqC4X"
      },
      "source": [
        "## LAB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvOLJX0cqEzm"
      },
      "source": [
        "class LAB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.illuminants = \\\n",
        "    {\"A\": {'2': (1.098466069456375, 1, 0.3558228003436005),\n",
        "           '10': (1.111420406956693, 1, 0.3519978321919493)},\n",
        "     \"D50\": {'2': (0.9642119944211994, 1, 0.8251882845188288),\n",
        "             '10': (0.9672062750333777, 1, 0.8142801513128616)},\n",
        "     \"D55\": {'2': (0.956797052643698, 1, 0.9214805860173273),\n",
        "             '10': (0.9579665682254781, 1, 0.9092525159847462)},\n",
        "     \"D65\": {'2': (0.95047, 1., 1.08883),   # This was: `lab_ref_white`\n",
        "             '10': (0.94809667673716, 1, 1.0730513595166162)},\n",
        "     \"D75\": {'2': (0.9497220898840717, 1, 1.226393520724154),\n",
        "             '10': (0.9441713925645873, 1, 1.2064272211720228)},\n",
        "     \"E\": {'2': (1.0, 1.0, 1.0),\n",
        "           '10': (1.0, 1.0, 1.0)}}\n",
        "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    def _get_xyz_coords(self,illuminant,observer):\n",
        "        \"\"\" Get the XYZ coordinates from illuminant and observer\n",
        "        Parameters\n",
        "        ==========\n",
        "        illuminant : {\"A\",\"D50\",\"D65\",\"D75\",\"E\"}\n",
        "        observer : {\"2\",\"10\"}\n",
        "        Returns\n",
        "        ==========\n",
        "        XYZ coordinate Tensor Float\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return torch.tensor(self.illuminants[illuminant][observer]).float()\n",
        "        except KeyError:\n",
        "            raise ValueError(\"Unknown illuminat:'{}'/observer:'{}' combination\".format(illuminant,observer))\n",
        "\n",
        "    def _check_shape(self,tensor):\n",
        "        if tensor.shape[0] != 3:\n",
        "            raise ValueError(\"Input array must have (batch, 3,height,width)\")\n",
        "\n",
        "    def xyz2rgb(self,xyz_tensor,show_results=False):\n",
        "        \"\"\"XYZ to RGB color space conversion.\n",
        "        Parameters\n",
        "        ==========\n",
        "        xyz_tensor : shape -> (3,height,width) Tensor\n",
        "        show_results : whether to display the resulting rgb image\n",
        "        Returns\n",
        "        ==========\n",
        "        rgb_tensor : shape -> (3,height,width) Tensor\n",
        "        \"\"\"\n",
        "        xyz_tensor = xyz_tensor.permute(1,2,0)\n",
        "        xyz_from_rgb = torch.tensor([[0.412453, 0.357580, 0.180423],\n",
        "                                     [0.212671, 0.715160, 0.072169],\n",
        "                                     [0.019334, 0.119193, 0.950227]]).to(self.device)\n",
        "        rgb_from_xyz = torch.inverse(xyz_from_rgb)\n",
        "        rgb = torch.matmul(xyz_tensor,torch.t(rgb_from_xyz))\n",
        "        mask = rgb > 0.0031308\n",
        "        rgb[mask] = 1.055 * torch.pow(rgb[mask],1/2.4) - 0.055\n",
        "        rgb[~mask] *= 12.92\n",
        "        rgb = torch.clamp(rgb,0,1)\n",
        "        rgb = rgb.permute(2,0,1)\n",
        "        if show_results:\n",
        "            rgb_numpy = rgb.cpu().detach().numpy().transpose(1,2,0)\n",
        "            plt.imshow(rgb_numpy)\n",
        "            plt.show()\n",
        "        return rgb\n",
        "\n",
        "    def lab2xyz(self,lab_tensor,show_results=False,illuminant='D65',observer='2'):\n",
        "        \"\"\"LAB to XYZ color space conversion.\n",
        "        Parameters\n",
        "        ==========\n",
        "        lab_tensor : shape -> (3,height,width) Tensor\n",
        "        show_results : whether to display the resulting xyz image\n",
        "        Returns\n",
        "        ==========\n",
        "        xyz_tensor : shape -> (3,height,width) Tensor\n",
        "        \"\"\"\n",
        "        l,a,b = lab_tensor[0],lab_tensor[1],lab_tensor[2]\n",
        "        y = (l+16.)/116.\n",
        "        x = (a / 500.) + y\n",
        "        z = y - (b / 200.)\n",
        "\n",
        "        xyz = torch.stack([x,y,z],dim=0)\n",
        "        mask = xyz > 0.2068966\n",
        "        xyz[mask] = torch.pow(xyz[mask],3.)\n",
        "        xyz[~mask] = (xyz[~mask] - 16. / 116.) / 7.787\n",
        "\n",
        "        xyz_ref_white = self._get_xyz_coords(illuminant,observer).to(self.device)\n",
        "        xyz = xyz.permute(1,2,0)\n",
        "        xyz *= xyz_ref_white\n",
        "        xyz = xyz.permute(2,0,1)\n",
        "        if show_results:\n",
        "            xyz_numpy = xyz.cpu().detach().numpy().transpose(1,2,0)\n",
        "            plt.imshow(xyz_numpy)\n",
        "            plt.show()\n",
        "        return xyz\n",
        "\n",
        "    def lab2rgb(self,lab_tensor,show_results_xyz=False,show_results_rgb=False):\n",
        "        \"\"\"LAB to RGB color space conversion.\n",
        "        Parameters\n",
        "        ==========\n",
        "        lab_tensor : shape -> (3,height,width) Tensor\n",
        "        show_results_xyz : whether to display the resulting xyz image\n",
        "        show_results_rgb : whether to display the resulting rgb image\n",
        "\n",
        "        Returns\n",
        "        ==========\n",
        "\n",
        "        rgb_tensor : shape -> (3,height,width) Tensor\n",
        "\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for i in range(lab_tensor.shape[0]):\n",
        "            xyz = self.lab2xyz(lab_tensor[i],show_results_xyz)\n",
        "            rgb = self.xyz2rgb(xyz,show_results_rgb)\n",
        "            results.append(rgb)\n",
        "        results = torch.cat(results).reshape(len(results),*results[0].shape)\n",
        "        return results\n",
        "\n",
        "    def rgb2xyz(self,rgb_tensor,show_results=False):\n",
        "        \"\"\"RGB to XYZ color space conversion.\n",
        "        Parameters\n",
        "        ==========\n",
        "        rgb_tensor : shape -> (3,height,width) Tensor\n",
        "        show_results : whether to display the resulting xyz image\n",
        "        Returns\n",
        "        ==========\n",
        "        xyz_tensor : shape -> (3,height,width) Tensor\n",
        "        what is xyz_tensor?\n",
        "        -------------------\n",
        "            -> https://www.dic-color.com/knowledge/xyz.html \n",
        "        \"\"\"\n",
        "        self._check_shape(rgb_tensor) #must have input shape {3,height,width}\n",
        "        rgb_tensor = rgb_tensor.permute(1,2,0)\n",
        "        mask = rgb_tensor > 0.04045\n",
        "        rgb_tensor[mask] = torch.pow((rgb_tensor[mask] + 0.055)/1.055,2.4)\n",
        "        rgb_tensor[~mask] /= 12.92\n",
        "        xyz_from_rgb = torch.tensor([[0.412453, 0.357580, 0.180423],\n",
        "                                     [0.212671, 0.715160, 0.072169],\n",
        "                                     [0.019334, 0.119193, 0.950227]]).to(self.device)\n",
        "        xyz = torch.matmul(rgb_tensor,torch.t(xyz_from_rgb))\n",
        "        if show_results: # show matplotlib\n",
        "            xyz_numpy = xyz.cpu().detach().numpy()\n",
        "            plt.imshow(xyz_numpy)\n",
        "            plt.show()\n",
        "\n",
        "        xyz = xyz.permute(2,0,1)\n",
        "        return xyz\n",
        "\n",
        "    def xyz2lab(self,xyz_tensor,show_results=False,illuminant='D65',observer='2'):\n",
        "        \"\"\"XYZ to CIE-LAB color space conversion.\n",
        "        Parameters\n",
        "        ==========\n",
        "        xyz_tensor : shape -> (3,height,width) Tensor\n",
        "        show_results : whether to display the resulting lab image\n",
        "        Returns\n",
        "        ==========\n",
        "        lab_tensor : shape -> (3,height,width) Tensor\n",
        "        \n",
        "        what is lab_tensor?\n",
        "        -------------------\n",
        "            -> http://rysys.co.jp/dpex/help_laboutput.html \n",
        "\n",
        "        \"\"\"\n",
        "        xyz_tensor = xyz_tensor.permute(1,2,0)\n",
        "\n",
        "        xyz_ref_white = self._get_xyz_coords(illuminant,observer).to(self.device)\n",
        "        xyz_tensor = xyz_tensor / xyz_ref_white\n",
        "\n",
        "        mask = xyz_tensor > 0.008856\n",
        "        xyz_tensor[mask] = torch.pow(xyz_tensor[mask],1/3)\n",
        "        xyz_tensor[~mask] = 7.787 * xyz_tensor[~mask] + 16. / 116.\n",
        "        x,y,z = xyz_tensor[...,0],xyz_tensor[...,1],xyz_tensor[...,2]\n",
        "        L = (116. * y) - 16.\n",
        "        a = 500. * (x - y)\n",
        "        b = 200. * (y - z)\n",
        "        lab = torch.cat([L.unsqueeze(-1),a.unsqueeze(-1),b.unsqueeze(-1)],dim=-1)\n",
        "        if show_results:\n",
        "            lab_numpy = lab.cpu().detach().numpy()\n",
        "            plt.imshow(lab_numpy)\n",
        "            plt.show()\n",
        "\n",
        "        lab = lab.permute(2,0,1)\n",
        "        return lab\n",
        "\n",
        "    def forward(self,rgb_tensor,show_xyz_results=False,show_lab_results=False):\n",
        "        results = []\n",
        "        for i in range(rgb_tensor.shape[0]):\n",
        "            xyz = self.rgb2xyz(rgb_tensor[i],show_xyz_results)\n",
        "            lab = self.xyz2lab(xyz,show_lab_results)\n",
        "            results.append(lab)\n",
        "        results = torch.cat(results).reshape(len(results),*results[0].shape)\n",
        "        return results\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4QeiGrurivk"
      },
      "source": [
        "## Parts to use in CWAN_L and CWAN_AB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHfzDS92rqcu"
      },
      "source": [
        "class MemoryBlock(nn.Module):\n",
        "    def __init__(self,channels,num_resblock,num_memblock):\n",
        "        super().__init__()\n",
        "        self.recursive_unit = nn.ModuleList(\n",
        "                [ResidualBlock(channels) for i in range(num_resblock)]\n",
        "        )\n",
        "        self.gate_unit = ReLUConv((num_resblock+num_memblock)*channels,channels,1,1,0)\n",
        "    def forward(self,x,ys):\n",
        "        xs = []\n",
        "        residual = x\n",
        "        for layer in self.recursive_unit:\n",
        "            x = layer(x)\n",
        "            xs.append(x)\n",
        "        gate_output = self.gate_unit(torch.cat(xs+ys,1))\n",
        "        ys.append(gate_output)\n",
        "        return gate_output\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self,channels,k=3,s=1,p=1):\n",
        "        super().__init__()\n",
        "        self.relu_conv1 = BNReLUConv(channels,channels,k,s,p)\n",
        "        self.relu_conv2 = BNReLUConv(channels,channels,k,s,p)\n",
        "    def forward(self,x):\n",
        "        residual = x\n",
        "        out = self.relu_conv1(x)\n",
        "        out = self.relu_conv2(out)\n",
        "        out = out + residual\n",
        "        return out\n",
        "\n",
        "class ReLUConv(nn.Sequential):\n",
        "    def __init__(self,in_channels,channels,k=3,s=1,p=1,inplace=True):\n",
        "        super().__init__()\n",
        "        self.add_module('relu',nn.ReLU(inplace=inplace))\n",
        "        self.add_module('conv',nn.Conv2d(in_channels,channels,k,s,p,bias=False))\n",
        "\n",
        "class BNReLUConv(nn.Sequential):\n",
        "    def __init__(self,in_channels,channels,k=3,s=1,p=1,inplace=True):\n",
        "        super().__init__()\n",
        "        self.add_module('bn',nn.BatchNorm2d(in_channels))\n",
        "        self.add_module('relu',nn.ReLU(inplace=inplace))\n",
        "        self.add_module('conv',nn.Conv2d(in_channels,channels,k,s,p,bias=False))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYfh9lNjrQWm"
      },
      "source": [
        "## CWAN_L"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8quwpf7trPnX"
      },
      "source": [
        "class CWAN_L(nn.Module):\n",
        "    \"\"\" 'L' of LAB's model.\n",
        "    motivation\n",
        "    ==========\n",
        "    focus on enhancing image lightness and denoising\n",
        "    name\n",
        "    ====\n",
        "    k -> kernel_size\n",
        "    n -> output channels size\n",
        "    x -> repeat number\n",
        "    parameters\n",
        "    =========\n",
        "    k3n32 -> memory blocks. this block utilize local short skip connections whitin the bloack to represent short-term memory,as well as long skip connections sourcing from previous blocks to represent long-term memory.\n",
        "    returns\n",
        "    =======\n",
        "    enhanced lightness image (1xHxW)\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = ReLUConv(1,32)\n",
        "        self.k3n1 = nn.Sequential(\n",
        "                nn.Conv2d(32,1,(3,3),stride=1,padding=1)\n",
        "        )\n",
        "        self.memory_blocks = nn.ModuleList(\n",
        "                [MemoryBlock(32,3,i+1) for i in range(3)]\n",
        "        )\n",
        "\n",
        "    def forward(self,l):\n",
        "        residual = l\n",
        "        out = self.feature_extractor(l)\n",
        "        ys = [out]\n",
        "        for memory_block in self.memory_blocks:\n",
        "            out = memory_block(out,ys)\n",
        "        out = self.k3n1(out)\n",
        "        out = out + residual\n",
        "        return out"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DnKn6x6pLlP"
      },
      "source": [
        "## CWAN_AB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpre-zVYp-LW"
      },
      "source": [
        "class CWAN_AB(nn.Module):\n",
        "    \"\"\" 'AB' of LAB's model.\n",
        "    motivation\n",
        "    ==========\n",
        "    color infomation drive the attention of CWAN_AB\n",
        "    name\n",
        "    ====\n",
        "    k -> kernel_size\n",
        "    n -> output channels size\n",
        "    parameters\n",
        "    ==========\n",
        "    returns\n",
        "    =======\n",
        "    1.enhanced color images(2xHxW)\n",
        "    2.color attention maps(2xHxW)\n",
        "    3.sparse attention points(2xHxW)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.k3n32_1 = nn.Sequential(\n",
        "                nn.Conv2d(2,32,(3,3),stride=1,padding=1),\n",
        "                nn.ReLU()\n",
        "        )\n",
        "        self.k3n32_2 = nn.Sequential(\n",
        "                nn.Conv2d(2,32,(3,3),stride=1,padding=1),\n",
        "                nn.ReLU()\n",
        "        )\n",
        "        k3n64_k1n128_k3n64 = nn.Sequential(\n",
        "                nn.Conv2d(32,64,(3,3),stride=1,padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(64,128,(1,1),stride=1,padding=0),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(128,64,(3,3),stride=1,padding=1),\n",
        "                nn.ReLU()\n",
        "        )\n",
        "        self.k3n64_k1n128_k3n64_1 = k3n64_k1n128_k3n64\n",
        "        self.k3n64_k1n128_k3n64_2 = k3n64_k1n128_k3n64\n",
        "        self.k3n64_k1n128_k3n64_3 = nn.Sequential(\n",
        "                nn.Conv2d(64,64,(3,3),stride=1,padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(64,128,(1,1),stride=1,padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(128,64,(3,3),stride=1,padding=1),\n",
        "                nn.ReLU()\n",
        "        )\n",
        "        self.k3n2 = nn.Sequential(\n",
        "                nn.Conv2d(64,2,(3,3),stride=1,padding=1)\n",
        "        )\n",
        "        self.k3n4 = nn.Sequential(\n",
        "                nn.Conv2d(64,4,(3,3))\n",
        "        )\n",
        "    def forward(self,ab):\n",
        "        residual = ab\n",
        "        k3n32_1_output = self.k3n32_1(ab)\n",
        "        k3n64_k1n128_k3n64_1_output = self.k3n64_k1n128_k3n64_1(k3n32_1_output)\n",
        "        k3n2_output = self.k3n2(k3n64_k1n128_k3n64_1_output)\n",
        "        attention_map = k3n2_output\n",
        "        k3n32_2_output = self.k3n32_2(residual + k3n2_output)\n",
        "        k3n64_k1n128_k3n64_2_output = self.k3n64_k1n128_k3n64_2(k3n32_2_output)\n",
        "        k3n64_k1n128_k3n64_3_output = self.k3n64_k1n128_k3n64_3(k3n64_k1n128_k3n64_2_output)\n",
        "        k3n4_output = self.k3n4(k3n64_k1n128_k3n64_3_output)\n",
        "        attention_points = k3n4_output[:,2:]\n",
        "        enhance_ab = residual + k3n4_output[:,:2]\n",
        "        return enhance_ab,attention_map,attention_points"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5qpetjrr2Vk"
      },
      "source": [
        "# CWAN Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2k5-0qApKvx"
      },
      "source": [
        "class CWAN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cwan_l = CWAN_L()\n",
        "        self.cwan_ab = CWAN_AB()\n",
        "        self.lab_converter = LAB()\n",
        "    def lab2rgb(self,lab):\n",
        "        \"\"\" LAB tensor to RGB tensor\n",
        "        Parameter\n",
        "        =========\n",
        "        lab : LAB image tensor\n",
        "        Returns\n",
        "        =========\n",
        "        rgb : RGB image tensor\n",
        "        \"\"\"\n",
        "        rgb = self.lab_converter.lab2rgb(lab)\n",
        "        return rgb\n",
        "\n",
        "    def l_test(self,tensor):\n",
        "        lab = self.lab_converter(tensor)\n",
        "        l = lab[:,:1]\n",
        "        l_output = self.cwan_l(l)\n",
        "        return l_output\n",
        "\n",
        "    def ab_test(self,tensor):\n",
        "        lab = self.lab_converter(tensor)\n",
        "        ab = lab[:,1]\n",
        "        ab_output,_,_ = self.cwan_ab(ab)\n",
        "        return ab_output\n",
        "\n",
        "    def forward(self,tensor):\n",
        "        lab = self.lab_converter(tensor)\n",
        "        l,ab = lab[:,:1],lab[:,1:]\n",
        "        l_output = self.cwan_l(l)\n",
        "        ab_output,attention_map,attention_points = self.cwan_ab(ab)\n",
        "        generated_image = torch.rand(tensor.shape)\n",
        "        generated_image[:,0] = l_output[:,0]\n",
        "        generated_image[:,1] = ab_output[:,0]\n",
        "        generated_image[:,2] = ab_output[:,1]\n",
        "        return generated_image,attention_map,attention_points,l_output,ab_output"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf5LMPydpT7L"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HfbQspusnrc"
      },
      "source": [
        "#training network\n",
        "cwan = CWAN()\n",
        "if _CWAN_L_PATH is not None:\n",
        "  cwan.cwan_l.load_state_dict(torch.load(_CWAN_L_PATH))\n",
        "if _CWAN_AB_PATH is not None:\n",
        "  cwan.cwan_ab.load_state_dict(torch.load(_CWAN_AB_PATH))\n",
        "cwan = cwan.train().to(device)\n",
        "lab = LAB()\n",
        "lab = lab.eval().to(device)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1cXRZtTnf9n"
      },
      "source": [
        "#training setting\n",
        "optimizer = torch.optim.Adam([{'params':cwan.cwan_ab.parameters()}],lr=_LR,weight_decay=_WEIGHT_DECAY)\n",
        "loss_func = nn.L1Loss()\n",
        "loss_mse_func = nn.MSELoss()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujPLmUKJo5wH"
      },
      "source": [
        "long_dic = dict()\n",
        "#loss data lists\n",
        "loss_list = list()\n",
        "loss_map_list = list()\n",
        "loss_huber_list = list()\n",
        "loss_mse_list = list()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQJukks2ohys"
      },
      "source": [
        "for e in tqdm(range(_START_EPOCH,_EPOCH)):\n",
        "  print(\"now {} epoch\".format(e))\n",
        "  print(\"+++++++++++++++++++++++++\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}